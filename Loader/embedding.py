import os
import sys
import pickle
from transformers import AutoTokenizer, AutoModel
import torch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Add the parent directory to the system path to import PDFLoader
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from load_pdf import PDFLoader


class EmbeddingGenerator:
    def __init__(
        self, model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    ):
        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)

    def generate_embeddings(self, chunks):
        """
        Generate embeddings for the given chunks of text.
        """
        chunk_texts = [chunk for chunk in chunks]
        chunk_vectors = self.embeddings.embed_documents(chunk_texts)
        return chunk_vectors

    def process_pkl_files(self, pkl_files):
        """
        Process all .pkl files and generate embeddings for each.
        """
        all_chunks = []
        for pkl_file in pkl_files:
            with open(pkl_file, "rb") as f:
                chapters_with_articles = pickle.load(f)
            for chapter in chapters_with_articles:
                for article in chapter:
                    print(type(article))
                    print(len(article))
                    if isinstance(article, tuple):
                        article = article[0]  # Extract the string from the tuple
                    chunks = loader.chunk_text(article)
                    all_chunks.extend(chunks)
        return all_chunks

    def save_faiss_index(self, chunks, faiss_index_dir):
        """
        Save the FAISS index to a directory.
        """
        if os.path.exists(faiss_index_dir):
            # Load the existing FAISS index (embeddings generation is skipped)
            faiss_index = FAISS.load_local(
                faiss_index_dir, self.embeddings, allow_dangerous_deserialization=True
            )
            print("Loaded existing FAISS index from", faiss_index_dir)
        else:
            # If index doesn't exist, generate embeddings and build the index
            print("Generating embeddings for chunks...")
            chunk_vectors = self.generate_embeddings(chunks)
            text_embeddings = list(zip(chunks, chunk_vectors))

            # Debug print to check if text_embeddings is empty
            print(f"Number of text embeddings: {len(text_embeddings)}")

            # Note: The following line computes embeddings and builds the FAISS index
            faiss_index = FAISS.from_embeddings(text_embeddings, self.embeddings)

            faiss_index.save_local(faiss_index_dir)
            print("FAISS index built and saved to", faiss_index_dir)


if __name__ == "__main__":
    # Initialize PDFLoader and process the dataset
    loader = PDFLoader()
    dataset_path = "../Dataset/"
    loader.process_dataset(dataset_path)
    current_folder_path = os.path.dirname(os.path.abspath(__file__))
    # Collect all .pkl files generated by PDFLoader
    pkl_files = [
        os.path.join(current_folder_path, f)
        for f in os.listdir(current_folder_path)
        if f.endswith(".pkl")
    ]

    # Initialize EmbeddingGenerator and generate embeddings for all .pkl files
    embedding_generator = EmbeddingGenerator()
    all_chunks = embedding_generator.process_pkl_files(pkl_files)
    chunk_vectors = embedding_generator.embeddings.embed_documents(all_chunks)
    # Debug print to check if all_chunks is empty
    print(f"Number of chunks: {len(all_chunks)}")
    text_embeddings = list(zip([chunk for chunk in all_chunks], chunk_vectors))
    # Save the FAISS index to a directory
    faiss_index_dir = "faiss_index"
    embedding_generator.save_faiss_index(all_chunks, faiss_index_dir)

    print("Finished")
