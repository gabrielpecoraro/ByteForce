import os
from mistralai import Mistral
from langchain_community.vectorstores import FAISS
import time
from tqdm import tqdm  # Make sure to import tqdm at the top


# Add the parent directory to the system path to import PDFLoader
from mistralai import Mistral
from sentence_transformers import SentenceTransformer

class EmbeddingGenerator:
    def __init__(self, api_key=None, model_name="mistral"):
        self.model_name = model_name
        
        if "mistral" in model_name.lower():
            if api_key is None:
                raise ValueError("API key must be provided for Mistral models.")
            self.client = Mistral(api_key=api_key)
            self.model_type = "mistral"
        else:
            # Default local model (sentence-transformers)
            self.client = SentenceTransformer(model_name)
            self.model_type = "local"

    def generate_embeddings(self, chunks, delay=1, max_retries=3, batch_size=32):
        chunk_vectors = []

        if self.model_type == "mistral":
            for chunk in tqdm(chunks, desc="Generating Mistral Embeddings"):
                retries = 0
                while True:
                    try:
                        response = self.client.embeddings.create(
                            model=self.model_name,
                            inputs=chunk,
                        )
                        chunk_vectors.append(response.data[0].embedding)
                        break
                    except Exception as e:
                        if "429" in str(e) or "rate limit" in str(e).lower():
                            retries += 1
                            if retries > max_retries:
                                raise Exception(f"Max retries exceeded for chunk: {chunk[:30]}") from e
                            time.sleep(delay)
                        else:
                            raise e

        elif self.model_type == "local":
            for i in tqdm(range(0, len(chunks), batch_size), desc="Generating embeddings"):
                batch = chunks[i:i + batch_size]
                embeddings = self.client.encode(batch)
                chunk_vectors.extend(embeddings)

        return chunk_vectors







    def save_faiss_index(self, texts, metadatas, faiss_index_dir):
        """
        Generate embeddings and save them into a FAISS index along with metadata.
        """
        if os.path.exists(faiss_index_dir):
            faiss_index = FAISS.load_local(
                faiss_index_dir, self.client.encode, allow_dangerous_deserialization=True
            )
            print("Loaded existing FAISS index from", faiss_index_dir)
        else:
            embeddings = self.generate_embeddings(texts)
            text_with_metadata = list(zip(texts, metadatas))
            faiss_index = FAISS.from_embeddings(text_with_metadata, embeddings)
            faiss_index.save_local(faiss_index_dir)
            print("FAISS index built and saved to", faiss_index_dir)


# if __name__ == "__main__":
#     # Initialize PDFLoader and process the dataset
#     loader = PDFLoader()
#     dataset_path = "../Dataset/"
#     loader.process_dataset(dataset_path)
#     current_folder_path = os.path.dirname(os.path.abspath(__file__))
#     pkl_folder = os.path.join(dataset_path, "pkl")

#     # Collect all .pkl files generated by PDFLoader
#     pkl_files = [
#         os.path.join(pkl_folder, f)
#         for f in os.listdir(pkl_folder)
#         if f.endswith(".pkl")
#     ]

#     # Initialize EmbeddingGenerator and generate embeddings for all .pkl files
#     # api_key = os.environ["CPHwxBTkpGr5svldVyrUr1aL21NgDDj7"]
#     embedding_generator = EmbeddingGenerator(api_key="CPHwxBTkpGr5svldVyrUr1aL21NgDDj7")
#     all_chunks = embedding_generator.process_pkl_files(pkl_files)
#     chunk_vectors = embedding_generator.generate_embeddings(all_chunks)
#     # Debug print to check if all_chunks is empty
#     print(f"Number of chunks: {len(all_chunks)}")
#     text_embeddings = list(zip([chunk for chunk in all_chunks], chunk_vectors))
#     # Save the FAISS index to a directory within the dataset folder
#     faiss_index_dir = os.path.join(dataset_path, "faiss_index")
#     embedding_generator.save_faiss_index(all_chunks, faiss_index_dir)

#     print("Finished")
