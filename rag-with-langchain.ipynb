{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10962684,"sourceType":"datasetVersion","datasetId":6820448},{"sourceId":11004143,"sourceType":"datasetVersion","datasetId":6850365}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T13:16:53.116896Z","iopub.execute_input":"2025-03-19T13:16:53.117162Z","iopub.status.idle":"2025-03-19T13:16:53.136662Z","shell.execute_reply.started":"2025-03-19T13:16:53.117142Z","shell.execute_reply":"2025-03-19T13:16:53.136033Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/europeanpatentofficev2/en-up-guidelines-2025-pre-publication.pdf\n/kaggle/input/qst-and-answer-oeb/Questions Sup OEB.docx\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:28:31.390991Z","iopub.execute_input":"2025-03-19T17:28:31.391319Z","iopub.status.idle":"2025-03-19T17:28:31.729758Z","shell.execute_reply.started":"2025-03-19T17:28:31.391291Z","shell.execute_reply":"2025-03-19T17:28:31.728941Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/qst-and-answer-oeb/Questions Sup OEB.docx\n/kaggle/input/europeanpatentofficev2/en-up-guidelines-2025-pre-publication.pdf\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:28:35.186760Z","iopub.execute_input":"2025-03-19T17:28:35.187174Z","iopub.status.idle":"2025-03-19T17:28:47.822942Z","shell.execute_reply.started":"2025-03-19T17:28:35.187151Z","shell.execute_reply":"2025-03-19T17:28:47.821728Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/415.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install python-docx\n!pip install -qU \"langchain[mistralai]\"\n!pip install -qU langchain-mistralai\n!pip install -qU langchain-chroma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:28:47.824997Z","iopub.execute_input":"2025-03-19T17:28:47.825404Z","iopub.status.idle":"2025-03-19T17:29:34.930886Z","shell.execute_reply.started":"2025-03-19T17:28:47.825374Z","shell.execute_reply":"2025-03-19T17:29:34.929910Z"}},"outputs":[{"name":"stdout","text":"Collecting python-docx\n  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\nDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-docx\nSuccessfully installed python-docx-1.1.2\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nimport getpass\nimport os\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain import hub\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langgraph.graph import START, StateGraph\nfrom typing_extensions import List, TypedDict\nfrom langchain.schema import Document as Doc_langshain\nfrom langchain.chat_models import init_chat_model\nfrom langchain_mistralai import MistralAIEmbeddings\nfrom langchain_chroma import Chroma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:00.267620Z","iopub.execute_input":"2025-03-19T17:39:00.267966Z","iopub.status.idle":"2025-03-19T17:39:00.273008Z","shell.execute_reply.started":"2025-03-19T17:39:00.267938Z","shell.execute_reply":"2025-03-19T17:39:00.271939Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"LANGSMITH_TRACING\"] = \"false\"\nos.environ[\"MISTRAL_API_KEY\"]   =  user_secrets.get_secret(\"MISTRAL_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:00.528771Z","iopub.execute_input":"2025-03-19T17:39:00.529081Z","iopub.status.idle":"2025-03-19T17:39:00.661035Z","shell.execute_reply.started":"2025-03-19T17:39:00.529057Z","shell.execute_reply":"2025-03-19T17:39:00.660307Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def split_doc_by_question(text):\n    \"\"\"\n    Découpe un document en sections à partir du mot-clé 'Question' suivi d'un numéro.\n    Retourne une liste de sections (chaque question avec ses réponses et explications).\n    \"\"\"\n    # Regex qui capte chaque bloc depuis 'Question X' jusqu'à la question suivante ou la fin\n    pattern = re.compile(r'(Question\\s*\\d+.*?)(?=Question\\s*\\d+|\\Z)', re.DOTALL | re.IGNORECASE)\n    matches = pattern.findall(text)\n\n    return [m.strip() for m in matches]\n\nfrom docx import Document\n\n# Charger le document Word\ndoc = Document(\"/kaggle/input/qst-and-answer-oeb/Questions Sup OEB.docx\")\n\n# Extraire tout le texte en un seul string\nfull_text = \"\\n\".join([para.text for para in doc.paragraphs])\n\n# Découper en blocs\nquestions = split_doc_by_question(full_text)\n\n# Convertir les questions en objets Document\nquestions_as_documents = [\n    Doc_langshain(page_content=question, metadata={\"source\": \"questions\"})\n    for question in questions\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:01.511749Z","iopub.execute_input":"2025-03-19T17:39:01.512065Z","iopub.status.idle":"2025-03-19T17:39:01.598696Z","shell.execute_reply.started":"2025-03-19T17:39:01.512043Z","shell.execute_reply":"2025-03-19T17:39:01.597943Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\nvector_store = Chroma(embedding_function=embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:01.809544Z","iopub.execute_input":"2025-03-19T17:39:01.809874Z","iopub.status.idle":"2025-03-19T17:39:02.922780Z","shell.execute_reply.started":"2025-03-19T17:39:01.809848Z","shell.execute_reply":"2025-03-19T17:39:02.921785Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Ajouter les documents à vector_store\n_ = vector_store.add_documents(documents=questions_as_documents)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:02.923978Z","iopub.execute_input":"2025-03-19T17:39:02.924293Z","iopub.status.idle":"2025-03-19T17:39:36.011217Z","shell.execute_reply.started":"2025-03-19T17:39:02.924268Z","shell.execute_reply":"2025-03-19T17:39:36.010246Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Définir le prompt pour la réponse à la question\n# LangChain dispose d'un \"hub\" (un dépôt) où tu peux récupérer des modèles préétablis, \n# des prompts, ou d'autres ressources partagées.\nprompt = hub.pull(\"rlm/rag-prompt\")\nprompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:36.013176Z","iopub.execute_input":"2025-03-19T17:39:36.013452Z","iopub.status.idle":"2025-03-19T17:39:36.103620Z","shell.execute_reply.started":"2025-03-19T17:39:36.013429Z","shell.execute_reply":"2025-03-19T17:39:36.102794Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n  warnings.warn(\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Définir l'état de l'application avec un dictionnaire typé\nclass State(TypedDict):\n    question: str  # La question posée par l'utilisateur\n    context: List[Document]  # Contexte sous forme de liste de documents récupérés\n    answer: str # La réponse générée pour la question","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:36.104844Z","iopub.execute_input":"2025-03-19T17:39:36.105104Z","iopub.status.idle":"2025-03-19T17:39:36.108757Z","shell.execute_reply.started":"2025-03-19T17:39:36.105079Z","shell.execute_reply":"2025-03-19T17:39:36.107802Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def retrieve(state: State):\n    # Recherche de documents similaires à la question dans la base de données\n    retrieved_docs_with_scores = vector_store.similarity_search_with_score(state[\"question\"], k=3)\n    \n    # Extraire les documents récupérés\n    retrieved_docs = [doc for doc, score in retrieved_docs_with_scores]\n    \n    # Extraire les scores des documents récupérés\n    # distances = [score for doc, score in retrieved_docs_with_scores]\n    #print(\"Distances des documents récupérés :\", distances)  # Affichage optionnel des distances pour vérification\n    \n    return {\"context\": retrieved_docs}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:36.109611Z","iopub.execute_input":"2025-03-19T17:39:36.109880Z","iopub.status.idle":"2025-03-19T17:39:36.124224Z","shell.execute_reply.started":"2025-03-19T17:39:36.109850Z","shell.execute_reply":"2025-03-19T17:39:36.123317Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Fonction pour générer la réponse à la question en fonction du contexte\ndef generate(state: State):\n    # Joindre le contenu des documents du contexte pour former un bloc de texte\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n    \n    # Générer un message à partir du prompt avec la question et le contexte\n    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n\n    # Invoker le modèle de langage pour générer la réponse\n    response = llm.invoke(messages)\n    return {\"answer\": response.content}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:36.125261Z","iopub.execute_input":"2025-03-19T17:39:36.125609Z","iopub.status.idle":"2025-03-19T17:39:36.142330Z","shell.execute_reply.started":"2025-03-19T17:39:36.125577Z","shell.execute_reply":"2025-03-19T17:39:36.141573Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Compiler l'application et la tester\ngraph_builder = StateGraph(State).add_sequence([retrieve, generate])  # Ajouter les étapes à l'application\ngraph_builder.add_edge(START, \"retrieve\")  # Définir la première étape du graph comme \"retrieve\"\ngraph = graph_builder.compile() # Compiler le graph pour l'exécution","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:39:36.143261Z","iopub.execute_input":"2025-03-19T17:39:36.143567Z","iopub.status.idle":"2025-03-19T17:39:36.160223Z","shell.execute_reply.started":"2025-03-19T17:39:36.143538Z","shell.execute_reply":"2025-03-19T17:39:36.159571Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"response = graph.invoke({\n    \"question\": \n                        \n    \"\"\"An individual applicant from Spain filed a European patent application in Spanish and requested examination in Spanish.\n     The examination and designation fees are due by November 7, 2023. The applicant asks if they are eligible for a 20% reduction in the examination fee. \n     What is the correct answer?\n    \"\"\"                                            \n    })\nprint(response[\"answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:40:26.911401Z","iopub.execute_input":"2025-03-19T17:40:26.911781Z","iopub.status.idle":"2025-03-19T17:40:29.174551Z","shell.execute_reply.started":"2025-03-19T17:40:26.911753Z","shell.execute_reply":"2025-03-19T17:40:29.173704Z"}},"outputs":[{"name":"stdout","text":"The applicant is entitled to a reduction in the examination fee, but the reduction is not 20%. The correct reduction is 30% as per Article 14(1) RFees. This is because the applicant is a natural person and filed the application and requested examination in Spanish, fulfilling the requirements for the fee reduction.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NGROK : \n# Sur Kaggle Notebooks (ou tout autre environnement cloud type Google Colab), tu n’as pas d’accès direct aux ports de la machine, il faudra une passerelle.","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\nimport os\nimport tempfile\nimport streamlit as st\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_mistralai import MistralAIEmbeddings\nfrom langchain import hub\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.chat_models import init_chat_model\nfrom docx import Document\nimport re\n\n# Configuration pour Kaggle\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"LANGSMITH_TRACING\"] = \"false\"\nos.environ[\"MISTRAL_API_KEY\"]   =  user_secrets.get_secret(\"MISTRAL_API_KEY\")\n\n# Interface Streamlit\nst.set_page_config(page_title=\"Exemple RAG\", layout=\"centered\")\n\nst.title(\"📄 RAG Assistant\")\nst.markdown(\"### Posez des questions sur vos documents PDF!\")\n\n# Modèles avec cache\n@st.cache_resource\ndef load_models():\n    return (\n        init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\"),\n        MistralAIEmbeddings(model=\"mistral-embed\"),\n        Chroma(embedding_function=MistralAIEmbeddings(model=\"mistral-embed\"))\n    )\n\nllm, embeddings, vector_store = load_models()\n        \nwith st.spinner(\"Analyse ...\"):\n    # Charger le document Word\n    doc = Document(\"/kaggle/input/qst-and-answer-oeb/Questions Sup OEB.docx\")\n    \n    # Extraire tout le texte en un seul string\n    full_text = \"\\n\".join([para.text for para in doc.paragraphs])\n    pattern = re.compile(r'(Question\\s*\\d+.*?)(?=Question\\s*\\d+|\\Z)', re.DOTALL | re.IGNORECASE)\n    matches = pattern.findall(full_text)\n    splits =  [m.strip() for m in matches]\n    \n    from langchain.schema import Document\n\n    splits = [\n        Document(page_content=splt, metadata={\"source\": \"questions\"})\n        for splt in splits\n    ]\n\n    vector_store = Chroma.from_documents(\n        splits, \n        embeddings,\n        persist_directory=\"/kaggle/working/chroma_db\"\n    )\n    st.session_state.vector_store = vector_store\n    st.success(\"RAG prêt!\")\n\n# Interface de question\nif \"vector_store\" in st.session_state:\n    question = st.text_input(\"Votre question :\")\n    \n    if question:\n        with st.spinner(\"Recherche...\"):\n            #docs = st.session_state.vector_store.similarity_search(question, k=1)\n            #context = \"\\n\\n\".join(d.page_content for d in docs)\n            context = st.session_state.vector_store.similarity_search(question)\n            \n            response = llm.invoke(\n                hub.pull(\"rlm/rag-prompt\").format(\n                    question=question,\n                    context=context\n                )\n            )\n            \n            st.markdown(f\"**Réponse :**\\n{response.content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:43:04.360861Z","iopub.execute_input":"2025-03-19T17:43:04.361209Z","iopub.status.idle":"2025-03-19T17:43:04.368357Z","shell.execute_reply.started":"2025-03-19T17:43:04.361183Z","shell.execute_reply":"2025-03-19T17:43:04.367272Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!pip install -q streamlit langchain-text-splitters langchain-community langchain-chroma langchain-mistralai pypdf nest-asyncio pyngrok\n\nimport subprocess\nfrom pyngrok import ngrok\n\n# Configuration de Ngrok\nAUTH_TOKEN =    user_secrets.get_secret(\"AUTH_TOKEN\")# Remplace par ta clé Ngrok\nngrok.set_auth_token(AUTH_TOKEN)\n# Configuration du tunnel\npublic_url = ngrok.connect(8501).public_url\nprint(f\"Accès à l'application : {public_url}\")\n\n# Lancement de Streamlit en arrière-plan\n!streamlit run app.py --server.port 8501 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T17:43:08.628625Z","iopub.execute_input":"2025-03-19T17:43:08.628930Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\nAccès à l'application : https://ea96-104-198-6-8.ngrok-free.app                                     \n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.198.6.8:8501\u001b[0m\n\u001b[0m\n/usr/local/lib/python3.10/dist-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}