# import os
# from mistralai import Mistral
# from langchain_community.vectorstores import FAISS
# import time
# from tqdm import tqdm  # Make sure to import tqdm at the top


# # Add the parent directory to the system path to import PDFLoader
# from mistralai import Mistral
# from sentence_transformers import SentenceTransformer

# class EmbeddingGenerator:
#     def __init__(self, api_key=None, model_name="mistral"):
#         self.model_name = model_name

#         if "mistral" in model_name.lower():
#             if api_key is None:
#                 raise ValueError("API key must be provided for Mistral models.")
#             self.client = Mistral(api_key=api_key)
#             self.model_type = "mistral"
#         else:
#             # Default local model (sentence-transformers)
#             self.client = SentenceTransformer(model_name)
#             self.model_type = "local"

#     def generate_embeddings(self, chunks, delay=1, max_retries=3, batch_size=32):
#         chunk_vectors = []

#         if self.model_type == "mistral":
#             for chunk in tqdm(chunks, desc="Generating Mistral Embeddings"):
#                 retries = 0
#                 while True:
#                     try:
#                         response = self.client.embeddings.create(
#                             model=self.model_name,
#                             inputs=chunk,
#                         )
#                         chunk_vectors.append(response.data[0].embedding)
#                         break
#                     except Exception as e:
#                         if "429" in str(e) or "rate limit" in str(e).lower():
#                             retries += 1
#                             if retries > max_retries:
#                                 raise Exception(f"Max retries exceeded for chunk: {chunk[:30]}") from e
#                             time.sleep(delay)
#                         else:
#                             raise e

#         elif self.model_type == "local":
#             for i in tqdm(range(0, len(chunks), batch_size), desc="Generating embeddings"):
#                 batch = chunks[i:i + batch_size]
#                 embeddings = self.client.encode(batch)
#                 chunk_vectors.extend(embeddings)

#         return chunk_vectors


#     def save_faiss_index(self, texts, metadatas, faiss_index_dir):
#         """
#         Generate embeddings and save them into a FAISS index along with metadata.
#         """
#         if os.path.exists(faiss_index_dir):
#             faiss_index = FAISS.load_local(
#                 faiss_index_dir, self.client.encode, allow_dangerous_deserialization=True
#             )
#             print("Loaded existing FAISS index from", faiss_index_dir)
#         else:
#             embeddings = self.generate_embeddings(texts)
#             text_with_metadata = list(zip(texts, metadatas))
#             faiss_index = FAISS.from_embeddings(text_with_metadata, embeddings)
#             faiss_index.save_local(faiss_index_dir)
#             print("FAISS index built and saved to", faiss_index_dir)


# # if __name__ == "__main__":
# #     # Initialize PDFLoader and process the dataset
# #     loader = PDFLoader()
# #     dataset_path = "../Dataset/"
# #     loader.process_dataset(dataset_path)
# #     current_folder_path = os.path.dirname(os.path.abspath(__file__))
# #     pkl_folder = os.path.join(dataset_path, "pkl")

# #     # Collect all .pkl files generated by PDFLoader
# #     pkl_files = [
# #         os.path.join(pkl_folder, f)
# #         for f in os.listdir(pkl_folder)
# #         if f.endswith(".pkl")
# #     ]

# #     # Initialize EmbeddingGenerator and generate embeddings for all .pkl files
# #     # api_key = os.environ["CPHwxBTkpGr5svldVyrUr1aL21NgDDj7"]
# #     embedding_generator = EmbeddingGenerator(api_key="CPHwxBTkpGr5svldVyrUr1aL21NgDDj7")
# #     all_chunks = embedding_generator.process_pkl_files(pkl_files)
# #     chunk_vectors = embedding_generator.generate_embeddings(all_chunks)
# #     # Debug print to check if all_chunks is empty
# #     print(f"Number of chunks: {len(all_chunks)}")
# #     text_embeddings = list(zip([chunk for chunk in all_chunks], chunk_vectors))
# #     # Save the FAISS index to a directory within the dataset folder
# #     faiss_index_dir = os.path.join(dataset_path, "faiss_index")
# #     embedding_generator.save_faiss_index(all_chunks, faiss_index_dir)

# #     print("Finished")


import os
import time
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from mistralai import Mistral
from sentence_transformers import SentenceTransformer
from langchain.embeddings.base import Embeddings
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document


class EmbeddingGenerator:
    def __init__(self, api_key=None, model_name="mistral"):
        self.model_name = model_name

        if "mistral" in model_name.lower():
            if api_key is None:
                raise ValueError("API key must be provided for Mistral models.")
            self.client = Mistral(api_key=api_key)
            self.model_type = "mistral"
        else:
            self.client = SentenceTransformer(model_name)
            self.model_type = "local"

    def get_langchain_embedding_model(self):
        """Return a LangChain-compatible embedding model (for FAISS and RAG use)."""
        if self.model_type == "mistral":
            return MistralLangChainEmbedding(self.client, self.model_name)
        else:
            return HuggingFaceEmbeddings(model_name=self.model_name)

    def generate_embeddings(self, chunks, delay=1, max_retries=3, batch_size=32):
        chunk_vectors = []

        if self.model_type == "mistral":
            for chunk in tqdm(chunks, desc="Generating Mistral Embeddings"):
                retries = 0
                while True:
                    try:
                        response = self.client.embeddings.create(
                            model=self.model_name,
                            inputs=chunk,
                        )
                        chunk_vectors.append(response.data[0].embedding)
                        break
                    except Exception as e:
                        if "429" in str(e) or "rate limit" in str(e).lower():
                            retries += 1
                            if retries > max_retries:
                                raise Exception(
                                    f"Max retries exceeded for chunk: {chunk[:30]}"
                                ) from e
                            time.sleep(delay)
                        else:
                            raise e

        elif self.model_type == "local":
            print("Generating embeddings...")
            for i in tqdm(
                range(0, len(chunks), batch_size), desc="Generating embeddings"
            ):
                batch = chunks[i : i + batch_size]
                embeddings = self.client.encode(batch)
                chunk_vectors.extend(embeddings)

        return chunk_vectors

    def save_embeddings_to_faiss(self, chunks, save_path, metadata_list=None):
        if os.path.exists(save_path):
            print(f"FAISS index already exists at {save_path}. Loading...")
            vectorstore = FAISS.load_local(
                folder_path=save_path,
                embeddings=self.get_langchain_embedding_model(),
                allow_dangerous_deserialization=True,
            )
            return vectorstore

        print("Creating new FAISS index...")

        if metadata_list is None:
            metadata_list = [{} for _ in chunks]
        print("1")
        documents = [
            Document(page_content=chunk, metadata=meta)
            for chunk, meta in zip(chunks, metadata_list)
        ]
        print("2")
        vectorstore = FAISS.from_documents(
            documents, embedding=self.get_langchain_embedding_model()
        )
        vectorstore.save_local(save_path)
        print(f"Saved FAISS index to {save_path}")

        return vectorstore


class MistralLangChainEmbedding(Embeddings):
    def __init__(self, client, model_name):
        self.client = client
        self.model_name = model_name

    def embed_documents(self, texts):
        return [
            self.client.embeddings.create(model=self.model_name, inputs=text)
            .data[0]
            .embedding
            for text in texts
        ]

    def embed_query(self, text):
        return (
            self.client.embeddings.create(model=self.model_name, inputs=text)
            .data[0]
            .embedding
        )
