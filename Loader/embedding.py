import os
from mistralai import Mistral
from langchain_community.vectorstores import FAISS
import math
import time

# Add the parent directory to the system path to import PDFLoader





class EmbeddingGenerator:
    def __init__(self, api_key, model_name="mistral-embed"):
        self.client = Mistral(api_key=api_key)
        self.model_name = model_name

    

   


    def generate_embeddings(self, chunks, delay=1, max_retries=3):
        """
        Generates embeddings for a list of text chunks.

        Args:
            chunks (list[str]): List of text chunks to embed.
            max_tokens (int): Maximum tokens allowed for each chunk (unused in this snippet, 
                              but you might want to use it for truncation or splitting).
            delay (float): Delay in seconds between retries on rate limit errors.
            max_retries (int): Maximum number of retries for each chunk when a rate limit is hit.

        Returns:
            list: A list of embedding responses corresponding to the input text chunks.
        """
        chunk_vector = []
        for chunk in chunks:
            retries = 0
            while True:
                try:
                    # Call the embedding function from the API client.
                    response = self.client.embeddings.create(
                        model=self.model_name,
                        inputs=chunk,
                    )
                    # Append the response (adjust extraction if your API returns a key like "embedding")
                    chunk_vector.append(response)
                    break  # Break out of the retry loop once successful.
                except Exception as e:
                    # Check if the error message suggests a rate limit (HTTP 429)
                    if "429" in str(e) or "rate limit" in str(e).lower():
                        retries += 1
                        if retries > max_retries:
                            raise Exception(f"Max retries exceeded for chunk: {chunk}") from e
                        print(f"Rate limit exceeded. Retrying after {delay} seconds (attempt {retries}/{max_retries})...")
                        time.sleep(delay)
                    else:
                        # Raise any other exceptions that are not rate limit errors.
                        raise e
        return chunk_vector







    def save_faiss_index(self, chunks, faiss_index_dir):
        """
        Save the FAISS index to a directory.
        """
        if os.path.exists(faiss_index_dir):
            # Load the existing FAISS index (embeddings generation is skipped)
            faiss_index = FAISS.load_local(
                faiss_index_dir, self.embeddings, allow_dangerous_deserialization=True
            )
            print("Loaded existing FAISS index from", faiss_index_dir)
        else:
            # If index doesn't exist, generate embeddings and build the index
            print("Generating embeddings for chunks...")
            chunk_vectors = self.generate_embeddings(chunks)
            text_embeddings = list(zip(chunks, chunk_vectors))

            # Debug print to check if text_embeddings is empty
            print(f"Number of text embeddings: {len(text_embeddings)}")

            # Note: The following line computes embeddings and builds the FAISS index
            faiss_index = FAISS.from_embeddings(text_embeddings, self.embeddings)

            faiss_index.save_local(faiss_index_dir)
            print("FAISS index built and saved to", faiss_index_dir)


# if __name__ == "__main__":
#     # Initialize PDFLoader and process the dataset
#     loader = PDFLoader()
#     dataset_path = "../Dataset/"
#     loader.process_dataset(dataset_path)
#     current_folder_path = os.path.dirname(os.path.abspath(__file__))
#     pkl_folder = os.path.join(dataset_path, "pkl")

#     # Collect all .pkl files generated by PDFLoader
#     pkl_files = [
#         os.path.join(pkl_folder, f)
#         for f in os.listdir(pkl_folder)
#         if f.endswith(".pkl")
#     ]

#     # Initialize EmbeddingGenerator and generate embeddings for all .pkl files
#     # api_key = os.environ["CPHwxBTkpGr5svldVyrUr1aL21NgDDj7"]
#     embedding_generator = EmbeddingGenerator(api_key="CPHwxBTkpGr5svldVyrUr1aL21NgDDj7")
#     all_chunks = embedding_generator.process_pkl_files(pkl_files)
#     chunk_vectors = embedding_generator.generate_embeddings(all_chunks)
#     # Debug print to check if all_chunks is empty
#     print(f"Number of chunks: {len(all_chunks)}")
#     text_embeddings = list(zip([chunk for chunk in all_chunks], chunk_vectors))
#     # Save the FAISS index to a directory within the dataset folder
#     faiss_index_dir = os.path.join(dataset_path, "faiss_index")
#     embedding_generator.save_faiss_index(all_chunks, faiss_index_dir)

#     print("Finished")
