import os
import sys
import pickle
from mistralai import Mistral
from langchain_community.vectorstores import FAISS

# Add the parent directory to the system path to import PDFLoader
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from Loader.load_pdf import PDFLoader


class EmbeddingGenerator:
    def __init__(self, api_key, model_name="mistral-embed"):
        self.client = Mistral(api_key=api_key)
        self.model_name = model_name

    def generate_embeddings(self, chunks):
        """
        Generate embeddings for the given chunks of text using Mistral API.
        """
        chunk_texts = [chunk for chunk in chunks]
        embeddings_batch_response = self.client.embeddings.create(
            model=self.model_name,
            inputs=chunk_texts, 
        )
        chunk_vectors = embeddings_batch_response["embeddings"]
        return chunk_vectors, embeddings_batch_response

    def process_pkl_files(self, pkl_files, loader=PDFLoader()):
        """
        Process all .pkl files and generate embeddings for each.
        """
        all_chunks = []
        for pkl_file in pkl_files:
            with open(pkl_file, "rb") as f:
                chapters_with_articles = pickle.load(f)
            for chapter in chapters_with_articles:
                for article in chapter:
                    if isinstance(article, tuple):
                        article = article[0]  # Extract the string from the tuple
                    chunks = loader.chunk_text(article)
                    all_chunks.extend(chunks)
        return all_chunks

    def save_faiss_index(self, chunks, faiss_index_dir):
        """
        Save the FAISS index to a directory.
        """
        if os.path.exists(faiss_index_dir):
            # Load the existing FAISS index (embeddings generation is skipped)
            faiss_index = FAISS.load_local(
                faiss_index_dir, self.embeddings, allow_dangerous_deserialization=True
            )
            print("Loaded existing FAISS index from", faiss_index_dir)
        else:
            # If index doesn't exist, generate embeddings and build the index
            print("Generating embeddings for chunks...")
            chunk_vectors = self.generate_embeddings(chunks)
            text_embeddings = list(zip(chunks, chunk_vectors))

            # Debug print to check if text_embeddings is empty
            print(f"Number of text embeddings: {len(text_embeddings)}")

            # Note: The following line computes embeddings and builds the FAISS index
            faiss_index = FAISS.from_embeddings(text_embeddings, self.embeddings)

            faiss_index.save_local(faiss_index_dir)
            print("FAISS index built and saved to", faiss_index_dir)


# if __name__ == "__main__":
#     # Initialize PDFLoader and process the dataset
#     loader = PDFLoader()
#     dataset_path = "../Dataset/"
#     loader.process_dataset(dataset_path)
#     current_folder_path = os.path.dirname(os.path.abspath(__file__))
#     pkl_folder = os.path.join(dataset_path, "pkl")

#     # Collect all .pkl files generated by PDFLoader
#     pkl_files = [
#         os.path.join(pkl_folder, f)
#         for f in os.listdir(pkl_folder)
#         if f.endswith(".pkl")
#     ]

#     # Initialize EmbeddingGenerator and generate embeddings for all .pkl files
#     # api_key = os.environ["CPHwxBTkpGr5svldVyrUr1aL21NgDDj7"]
#     embedding_generator = EmbeddingGenerator(api_key="CPHwxBTkpGr5svldVyrUr1aL21NgDDj7")
#     all_chunks = embedding_generator.process_pkl_files(pkl_files)
#     chunk_vectors = embedding_generator.generate_embeddings(all_chunks)
#     # Debug print to check if all_chunks is empty
#     print(f"Number of chunks: {len(all_chunks)}")
#     text_embeddings = list(zip([chunk for chunk in all_chunks], chunk_vectors))
#     # Save the FAISS index to a directory within the dataset folder
#     faiss_index_dir = os.path.join(dataset_path, "faiss_index")
#     embedding_generator.save_faiss_index(all_chunks, faiss_index_dir)

#     print("Finished")
